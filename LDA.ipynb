{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "305b0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases, LdaModel\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b44f0e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vldth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20eb23e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n",
      "                                              review  sentiment\n",
      "0  at first gumagana cya..pero pagnalowbat cya nd...          1\n",
      "1  grabi pangalawa ko ng order sa shapee pero pur...          1\n",
      "2  2l gray/black order ko. bakit 850ml lang po pi...          1\n",
      "3  walang silbing product.. bwesit. di gumagana d...          1\n",
      "4  d po maganda naman po yung neck fan, pero po n...          4\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('dataset/SentiTaglish_ProductsAndServices.csv')\n",
    "print(\"Original dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04ac2df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review\n",
      "0  at first gumagana cya..pero pagnalowbat cya nd...\n",
      "1  grabi pangalawa ko ng order sa shapee pero pur...\n",
      "2  2l gray/black order ko. bakit 850ml lang po pi...\n",
      "3  walang silbing product.. bwesit. di gumagana d...\n",
      "4  d po maganda naman po yung neck fan, pero po n...\n"
     ]
    }
   ],
   "source": [
    "# Drop the sentiment column\n",
    "reviews_df = df.drop(columns=['sentiment'])\n",
    "print(reviews_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ec261ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = reviews_df['review'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "001e98d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tagalog stopwords function\n",
    "def load_stopwords(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return set(line.strip() for line in file if line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01e67728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "# Tagalog/Filipino stopwords \n",
    "tagalog_stopwords = load_stopwords(\"stopwords-tl.txt\")\n",
    "\n",
    "combined_stopwords = set(english_stopwords).union(tagalog_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "215e29fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_data(documents):\n",
    "    return [\n",
    "        [word for word in simple_preprocess(str(doc)) if word not in combined_stopwords]\n",
    "        for doc in documents\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3c0885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the documents\n",
    "processed_texts = preprocess_data(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3446b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigram and trigram models\n",
    "bigram = Phrases(processed_texts, min_count=3, threshold=5)\n",
    "trigram = Phrases(bigram[processed_texts], threshold=5)\n",
    "\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06329215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply phrase models\n",
    "def make_ngrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "processed_texts = make_ngrams(processed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0bfd68f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and corpus\n",
    "id2word = corpora.Dictionary(processed_texts)\n",
    "corpus = [id2word.doc2bow(text) for text in processed_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a38676bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train the LDA model\n",
    "num_topics = 10\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=num_topics,\n",
    "    random_state=42,\n",
    "    passes=20,\n",
    "    iterations=1000,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "195f2b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics found by LDA:\n",
      "[(0,\n",
      "  '0.018*\"food\" + 0.009*\"room\" + 0.008*\"place\" + 0.008*\"complete_orders\" + '\n",
      "  '0.008*\"staff\" + 0.007*\"mama\" + 0.007*\"medyo_matagal\" + 0.007*\"pcs\" + '\n",
      "  '0.007*\"purple\" + 0.007*\"tubig\"'),\n",
      " (1,\n",
      "  '0.033*\"gumagana\" + 0.017*\"working\" + 0.016*\"kuya_rider\" + '\n",
      "  '0.013*\"sayang_pera\" + 0.010*\"fan\" + 0.010*\"charge\" + 0.008*\"gumana\" + '\n",
      "  '0.007*\"bilis\" + 0.007*\"charger\" + 0.006*\"hangin\"'),\n",
      " (2,\n",
      "  '0.067*\"ganda\" + 0.033*\"size\" + 0.022*\"good_quality\" + 0.020*\"ulit\" + '\n",
      "  '0.019*\"maliit\" + 0.016*\"worth\" + 0.016*\"sakto\" + 0.012*\"manipis\" + '\n",
      "  '0.012*\"kasya\" + 0.010*\"malaki\"'),\n",
      " (3,\n",
      "  '0.015*\"sulit\" + 0.014*\"ganda_quality\" + 0.012*\"expect\" + '\n",
      "  '0.010*\"maganda_tela\" + 0.008*\"laki\" + 0.008*\"mainit\" + 0.007*\"pesos\" + '\n",
      "  '0.007*\"tumagal\" + 0.006*\"tumbler\" + 0.006*\"makapal_tela\"'),\n",
      " (4,\n",
      "  '0.019*\"battery\" + 0.017*\"far\" + 0.013*\"soon\" + 0.012*\"nagana\" + '\n",
      "  '0.010*\"good_condition\" + 0.009*\"sakto_size\" + 0.009*\"remote\" + '\n",
      "  '0.007*\"nilagay\" + 0.007*\"pants\" + 0.006*\"nadeliver\"'),\n",
      " (5,\n",
      "  '0.010*\"good_price\" + 0.009*\"sulit_price\" + 0.009*\"excellent\" + '\n",
      "  '0.008*\"ring_light\" + 0.007*\"bulsa\" + 0.007*\"excellent_quality\" + '\n",
      "  '0.007*\"dents\" + 0.006*\"add_size\" + 0.006*\"saktong_sakto\" + 0.006*\"madali\"'),\n",
      " (6,\n",
      "  '0.053*\"maganda\" + 0.025*\"good\" + 0.025*\"dumating\" + 0.015*\"items\" + '\n",
      "  '0.013*\"wala\" + 0.010*\"maayos\" + 0.010*\"color\" + 0.010*\"goods\" + '\n",
      "  '0.009*\"inorder\" + 0.009*\"shop\"'),\n",
      " (7,\n",
      "  '0.026*\"super_ganda\" + 0.014*\"tela\" + 0.012*\"rubber\" + 0.010*\"medyo_manipis\" '\n",
      "  '+ 0.010*\"bag\" + 0.008*\"sakto_price\" + 0.007*\"tama_kulay\" + 0.007*\"zipper\" + '\n",
      "  '0.007*\"cotton\" + 0.007*\"masarap\"'),\n",
      " (8,\n",
      "  '0.021*\"pwede\" + 0.017*\"affordable\" + 0.015*\"uulitin\" + '\n",
      "  '0.011*\"fast_delivery\" + 0.010*\"well_packed\" + 0.009*\"phone\" + '\n",
      "  '0.008*\"mabilis\" + 0.008*\"best\" + 0.007*\"plastic\" + 0.007*\"gaganda\"'),\n",
      " (9,\n",
      "  '0.021*\"price\" + 0.012*\"love\" + 0.011*\"much\" + 0.011*\"still\" + '\n",
      "  '0.011*\"packaging\" + 0.011*\"medyo\" + 0.010*\"time\" + 0.010*\"bilis_dumating\" + '\n",
      "  '0.010*\"buti\" + 0.010*\"nice\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the topics\n",
    "print(\"\\nTopics found by LDA:\")\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "062303dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score: 0.45241004896749104\n"
     ]
    }
   ],
   "source": [
    "# Compute coherence score\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=lda_model,\n",
    "    texts=processed_texts,\n",
    "    dictionary=id2word,\n",
    "    coherence='c_v'\n",
    ")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score:', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82505894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved bigrams/trigrams to bigrams.txt\n"
     ]
    }
   ],
   "source": [
    "# Step 8 (Optional): View most common n-grams\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# Get only tokens that are bigrams/trigrams (contain \"_\")\n",
    "ngram_tokens = list(itertools.chain.from_iterable(\n",
    "    [token for token in doc if '_' in token] for doc in processed_texts\n",
    "))\n",
    "\n",
    "# Count frequency\n",
    "ngram_counts = Counter(ngram_tokens)\n",
    "\n",
    "# Write to a file\n",
    "with open('bigrams.txt', 'w', encoding='utf-8') as f:\n",
    "    for phrase, count in ngram_counts.most_common():\n",
    "        f.write(f'{phrase}\\t{count}\\n')\n",
    "\n",
    "print(\"\\nSaved bigrams/trigrams to bigrams.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26e4c265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maganda: 1504\n",
      "good: 1033\n",
      "dumating: 883\n",
      "ganda: 664\n",
      "size: 563\n",
      "wala: 561\n",
      "items: 483\n",
      "quality: 458\n",
      "sira: 448\n",
      "color: 372\n",
      "inorder: 372\n",
      "maayos: 369\n",
      "mura: 361\n",
      "price: 352\n",
      "gumagana: 351\n",
      "nice: 332\n",
      "agad: 320\n",
      "shop: 309\n",
      "kulay: 304\n",
      "disappointed: 300\n",
      "damage: 300\n",
      "super: 290\n",
      "medyo: 284\n",
      "pinadala: 272\n",
      "sobrang: 270\n",
      "maliit: 255\n",
      "much: 251\n",
      "packaging: 249\n",
      "cute: 246\n",
      "black: 243\n",
      "good_quality: 237\n",
      "sayang_pera: 237\n",
      "sakin: 236\n",
      "ulit: 236\n",
      "manipis: 233\n",
      "goods: 232\n",
      "worth: 231\n",
      "next_time: 218\n",
      "white: 216\n",
      "rider: 216\n",
      "amoy: 213\n",
      "tas: 213\n",
      "received: 212\n",
      "pwede: 212\n",
      "like: 209\n",
      "binigay: 207\n",
      "ba: 206\n",
      "akala: 204\n",
      "tama: 201\n",
      "ordered: 197\n",
      "working: 195\n",
      "man: 194\n",
      "sayang: 186\n",
      "picture: 184\n",
      "ganun: 184\n",
      "maganda_quality: 184\n",
      "design: 180\n",
      "ibang: 178\n",
      "sakto: 178\n",
      "mas: 176\n",
      "box: 175\n",
      "nagustuhan: 173\n",
      "time: 169\n",
      "deliver: 168\n",
      "mali: 164\n",
      "naka: 163\n",
      "malaki: 161\n",
      "delivery: 157\n",
      "parcel: 157\n",
      "baby: 157\n",
      "satisfied: 154\n",
      "star: 153\n",
      "bumili: 152\n",
      "plastic: 151\n",
      "alam: 147\n",
      "binili: 147\n",
      "buti: 146\n",
      "mabilis: 143\n",
      "pink: 142\n",
      "one: 142\n",
      "love: 142\n",
      "buy: 141\n",
      "need: 139\n",
      "try: 138\n",
      "battery: 138\n",
      "daw: 136\n",
      "anak: 134\n",
      "refund: 134\n",
      "bubble_wrap: 133\n",
      "lalo: 131\n",
      "since: 130\n",
      "pangit: 129\n",
      "fan: 129\n",
      "ayos: 129\n",
      "gamitin: 129\n",
      "namn: 128\n",
      "legit: 128\n",
      "still: 127\n",
      "super_ganda: 126\n",
      "expect: 124\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# Flatten list of tokenized docs\n",
    "all_words = list(itertools.chain.from_iterable(processed_texts))\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# View top N most common words\n",
    "top_words = word_freq.most_common(100)  # Top 50\n",
    "for word, freq in top_words:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
