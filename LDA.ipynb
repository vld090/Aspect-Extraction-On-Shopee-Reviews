{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "305b0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases, LdaModel\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b44f0e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vldth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "20eb23e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n",
      "                                              review  sentiment\n",
      "0  at first gumagana cya..pero pagnalowbat cya nd...          1\n",
      "1  grabi pangalawa ko ng order sa shapee pero pur...          1\n",
      "2  2l gray/black order ko. bakit 850ml lang po pi...          1\n",
      "3  walang silbing product.. bwesit. di gumagana d...          1\n",
      "4  d po maganda naman po yung neck fan, pero po n...          4\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('dataset/SentiTaglish_ProductsAndServices.csv')\n",
    "print(\"Original dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "04ac2df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review\n",
      "0  at first gumagana cya..pero pagnalowbat cya nd...\n",
      "1  grabi pangalawa ko ng order sa shapee pero pur...\n",
      "2  2l gray/black order ko. bakit 850ml lang po pi...\n",
      "3  walang silbing product.. bwesit. di gumagana d...\n",
      "4  d po maganda naman po yung neck fan, pero po n...\n"
     ]
    }
   ],
   "source": [
    "# Drop the sentiment column\n",
    "reviews_df = df.drop(columns=['sentiment'])\n",
    "print(reviews_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ec261ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = reviews_df['review'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "001e98d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tagalog stopwords function\n",
    "def load_stopwords(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return set(line.strip() for line in file if line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "01e67728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "# Tagalog/Filipino stopwords \n",
    "tagalog_stopwords = load_stopwords(\"stopwords-tl.txt\")\n",
    "\n",
    "combined_stopwords = set(english_stopwords).union(tagalog_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "215e29fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_data(documents):\n",
    "    return [\n",
    "        [word for word in simple_preprocess(str(doc)) if word not in combined_stopwords]\n",
    "        for doc in documents\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b3c0885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the documents\n",
    "processed_texts = preprocess_data(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3446b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigram and trigram models\n",
    "bigram = Phrases(processed_texts, min_count=3, threshold=5)\n",
    "trigram = Phrases(bigram[processed_texts], threshold=5)\n",
    "\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "06329215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply phrase models\n",
    "def make_ngrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "processed_texts = make_ngrams(processed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0bfd68f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and corpus\n",
    "id2word = corpora.Dictionary(processed_texts)\n",
    "corpus = [id2word.doc2bow(text) for text in processed_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a38676bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train the LDA model\n",
    "num_topics = 10\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=num_topics,\n",
    "    random_state=42,\n",
    "    passes=20,\n",
    "    iterations=1000,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "195f2b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics found by LDA:\n",
      "[(0,\n",
      "  '0.081*\"po\" + 0.047*\"maganda\" + 0.029*\"seller\" + 0.025*\"ganda\" + '\n",
      "  '0.025*\"thank_seller\" + 0.025*\"good\" + 0.023*\"order\" + 0.022*\"thank\" + '\n",
      "  '0.019*\"item\" + 0.016*\"quality\"'),\n",
      " (1,\n",
      "  '0.013*\"ganda_po\" + 0.013*\"food\" + 0.013*\"plastic\" + 0.012*\"time\" + '\n",
      "  '0.012*\"bilis_dumating\" + 0.009*\"dumi\" + 0.008*\"expected\" + '\n",
      "  '0.008*\"god_bless\" + 0.008*\"malakas\" + 0.007*\"part\"'),\n",
      " (2,\n",
      "  '0.018*\"still\" + 0.015*\"like\" + 0.008*\"nagustohan\" + 0.008*\"anyway\" + '\n",
      "  '0.008*\"konting\" + 0.007*\"ringlight\" + 0.007*\"even\" + 0.007*\"thank_thank\" + '\n",
      "  '0.007*\"excellent_quality\" + 0.007*\"freebies\"'),\n",
      " (3,\n",
      "  '0.011*\"magaan\" + 0.009*\"lakas\" + 0.008*\"bet\" + 0.007*\"maingay\" + '\n",
      "  '0.007*\"pretty\" + 0.006*\"super_sulit\" + 0.006*\"handle\" + 0.006*\"solid\" + '\n",
      "  '0.006*\"nice_product\" + 0.005*\"matagal_dumating\"'),\n",
      " (4,\n",
      "  '0.008*\"god_bless_po\" + 0.008*\"mukhang_matibay\" + 0.008*\"mouse_pad\" + '\n",
      "  '0.007*\"jogger\" + 0.007*\"secure\" + 0.007*\"kapal\" + 0.007*\"basa\" + '\n",
      "  '0.006*\"socks\" + 0.006*\"mouse\" + 0.006*\"ibang_color\"'),\n",
      " (5,\n",
      "  '0.099*\"yung\" + 0.044*\"kaso\" + 0.026*\"kasi\" + 0.018*\"size\" + 0.012*\"parang\" '\n",
      "  '+ 0.010*\"price\" + 0.010*\"rin\" + 0.010*\"goods\" + 0.009*\"pag\" + 0.009*\"nung\"'),\n",
      " (6,\n",
      "  '0.025*\"amoy\" + 0.018*\"mabilis_dumating\" + 0.017*\"yung_tela\" + 0.016*\"legit\" '\n",
      "  '+ 0.014*\"kaso_maliit\" + 0.014*\"kuya_rider\" + 0.012*\"godbless\" + '\n",
      "  '0.012*\"thank_shopee\" + 0.012*\"far\" + 0.012*\"sobrang_ganda\"'),\n",
      " (7,\n",
      "  '0.046*\"order\" + 0.039*\"dumating\" + 0.021*\"color\" + 0.018*\"seller\" + '\n",
      "  '0.016*\"inorder\" + 0.015*\"tapos\" + 0.014*\"black\" + 0.013*\"kulay\" + '\n",
      "  '0.011*\"white\" + 0.010*\"pinadala\"'),\n",
      " (8,\n",
      "  '0.015*\"overall\" + 0.010*\"always\" + 0.008*\"mganda\" + 0.008*\"smooth\" + '\n",
      "  '0.008*\"cotton\" + 0.007*\"soon\" + 0.007*\"like\" + 0.007*\"saktong_sakto\" + '\n",
      "  '0.007*\"wall\" + 0.006*\"mascara\"'),\n",
      " (9,\n",
      "  '0.024*\"hehe\" + 0.021*\"thank_much\" + 0.019*\"sulit\" + 0.012*\"good_condition\" '\n",
      "  '+ 0.010*\"well_packed\" + 0.009*\"thank_much_seller\" + 0.009*\"malambot\" + '\n",
      "  '0.008*\"effective\" + 0.007*\"got\" + 0.007*\"hangin\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the topics\n",
    "print(\"\\nTopics found by LDA:\")\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "062303dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score: 0.5354085237039004\n"
     ]
    }
   ],
   "source": [
    "# Compute coherence score\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=lda_model,\n",
    "    texts=processed_texts,\n",
    "    dictionary=id2word,\n",
    "    coherence='c_v'\n",
    ")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score:', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "82505894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved bigrams/trigrams to bigrams.txt\n"
     ]
    }
   ],
   "source": [
    "# Step 8 (Optional): View most common n-grams\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# Get only tokens that are bigrams/trigrams (contain \"_\")\n",
    "ngram_tokens = list(itertools.chain.from_iterable(\n",
    "    [token for token in doc if '_' in token] for doc in processed_texts\n",
    "))\n",
    "\n",
    "# Count frequency\n",
    "ngram_counts = Counter(ngram_tokens)\n",
    "\n",
    "# Write to a file\n",
    "with open('bigrams.txt', 'w', encoding='utf-8') as f:\n",
    "    for phrase, count in ngram_counts.most_common():\n",
    "        f.write(f'{phrase}\\t{count}\\n')\n",
    "\n",
    "print(\"\\nSaved bigrams/trigrams to bigrams.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
